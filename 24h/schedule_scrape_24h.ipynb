{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "413797a5-85e5-4b6a-8c9e-73360fd74b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "from bs4 import BeautifulSoup,NavigableString, Comment, Tag\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import html5lib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5ee2e246-7446-485b-b337-a2b1057a7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing library\n",
    "from bs4 import BeautifulSoup,NavigableString, Comment, Tag\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import html5lib\n",
    "def remove_div(article):\n",
    "    divs = article.find_all('div')\n",
    "    empty_divs = [div for div in divs if not div.text.strip() and not div.contents]\n",
    "    if not empty_divs:\n",
    "        return  # No more empty divs, stop recursion\n",
    "    for div in empty_divs:\n",
    "        div.decompose()\n",
    "def get_content(url):\n",
    "    response = requests.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(response.content,  'html.parser')\n",
    "    try:\n",
    "        article = soup.find('article', class_ = 'cate-24h-foot-arti-deta-info')\n",
    "        #remove the last 4 element\n",
    "        try:\n",
    "            article.find('p', class_ = 'tuht_all').decompose()\n",
    "            article.find('div',class_  = 'bv-lq').decompose()\n",
    "            article.find('div', id = 'zone_banner_sponser_product').decompose()\n",
    "            #article.find('p', class_ = 'linkOrigin').decompose()\n",
    "            article.find('div', class_ ='pgS2 txtCent grnBxBg bld clrGrn mgt20').decompose()\n",
    "            article.find('div', class_ = 'block-quiz').decompose()\n",
    "            article.find('div', class_ = 'podcasts-eva-t').decompose()\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "        tags_to_remove = article.find_all(['span'])\n",
    "        for tag in tags_to_remove:\n",
    "            # Extract the text from the tag\n",
    "            tag_text = tag.get_text()\n",
    "            # Replace the tag with its text content\n",
    "            tag.replace_with(tag_text)\n",
    "            tag.text.strip()\n",
    "        a_tags_to_remove = article.find_all('a', class_ = \"TextlinkBaiviet\" )\n",
    "        for a in a_tags_to_remove:\n",
    "            # Extract the text from the tag\n",
    "            tag_text = a .get_text()\n",
    "            # Replace the tag with its text content\n",
    "            a .replace_with(tag_text)\n",
    "            a .text.strip()\n",
    "        # set attribut for img\n",
    "        # set attribut for img\n",
    "        b = article.find_all('img')\n",
    "        for i in b:\n",
    "            if '.svg' in i['src']:\n",
    "                i.decompose()\n",
    "        for i in article.find_all('img'):\n",
    "            if 'data-original' in i.attrs.keys():\n",
    "                i['src'] = i.get('data-original')\n",
    "            #i.attrs = ['class', 'alt', 'src', 'data-original']\n",
    "            del i['onclick']\n",
    "            del i['style']\n",
    "            del i['class']\n",
    "        #remove js  and css\n",
    "        h2 = soup.find('article', class_ = 'cate-24h-foot-arti-deta-info').find('h2')\n",
    "        h2_text = '\\n'.join(line.strip() for line in h2.get_text().split('\\n') if line.strip())\n",
    "        # Update the text of the h2 tag\n",
    "        h2.string = h2_text\n",
    "        #soup2 = BeautifulSoup(\"\", 'html.parser')\n",
    "        #soup2.append(h2)\n",
    "        for script_or_style in article(['script', 'style']):\n",
    "            script_or_style.decompose()\n",
    "        for i in article.find_all(recursive = True):\n",
    "            if i.name != 'p':\n",
    "                try:\n",
    "                    del i['onclick']\n",
    "                    del i['id']\n",
    "                    del i['class']\n",
    "                    del i['style']\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "                except TypeError:\n",
    "                    continue\n",
    "        for i in article.find_all('img'):\n",
    "            i['class'] = \"aligncenter\"\n",
    "            i['width'] = 800\n",
    "            i['height'] = 400\n",
    "        for video in article.find_all('video'):\n",
    "            video.decompose()\n",
    "        for i in article.find_all('p', class_ = 'img_chu_thich_0407'):\n",
    "                i['style'] = \"text-align: center;\"\n",
    "                del i['class']\n",
    "        for i in article.find_all('div', {'align': 'center'}):\n",
    "            i.decompose()\n",
    "        for element in article.find_all(recursive = True,string=True):\n",
    "            if isinstance(element, NavigableString) and element.strip() == '':\n",
    "                element.extract()\n",
    "        for i in article.find_all('div'):\n",
    "            if i.children == None and i.string == None:\n",
    "                i.decompose()\n",
    "        source_tag = soup.new_tag('i') \n",
    "        source_tag.string = \"Nguồn: 24h.com.vn\"  # Set the content of <i> tag\n",
    "        # Append the <i> tag as the last child of the <article> tag\n",
    "        article.append(source_tag)\n",
    "        \"\"\"a_tags_to_remove = article.find_all('a', class_ =\"TextlinkBaiviet\")\n",
    "        for a in a_tags_to_remove:\n",
    "            # Extract the text from the tag\n",
    "            tag_text = a .get_text()\n",
    "            # Replace the tag with its text content\n",
    "            a.replace_with(tag_text)\n",
    "            a.text.strip()\"\"\"\n",
    "        #Handling a tag \n",
    "        a_tags = soup.find_all('a')\n",
    "        for a_tag in a_tags:\n",
    "            # Check if the <a> tag has no child tags\n",
    "            if all(not isinstance(child, Tag) for child in a_tag.children):\n",
    "                # Convert <a> tag to its text if it has no child tags\n",
    "                a_tag.replace_with(a_tag.get_text())\n",
    "            else:\n",
    "                # If <a> tag has child elements, replace it with a <span> tag but keep the children\n",
    "                new_span = soup.new_tag(\"span\")\n",
    "                new_span.extend(a_tag.contents)  # Use extend to add all child elements\n",
    "                a_tag.replace_with(new_span)    \n",
    "        remove_div(article)\n",
    "        article.find('p', class_ = 'linkOrigin').decompose()\n",
    "    except (AttributeError, IndexError, TypeError):\n",
    "        try:\n",
    "            article = soup.find('div', id= 'magazine_news')\n",
    "            strong_text = article.find('div', class_ = 'titZing').find('p').text.strip()\n",
    "            source_tag = soup.new_tag('h2') \n",
    "            source_tag.string = strong_text\n",
    "            article.insert(0,source_tag)\n",
    "            article.find('div', class_ = 'titZing').decompose()\n",
    "            videos = article.find_all('div', class_ = 'videoUpload24h')\n",
    "            vd_captions = article.find_all('div', class_ = 'mg-video-caption')\n",
    "            for vd_caption in vd_captions:\n",
    "                vd_caption.decompose()\n",
    "            for video in videos:\n",
    "                video.decompose()\n",
    "            tags_to_remove = article.find_all(['span'])\n",
    "            for tag in tags_to_remove:\n",
    "                # Extract the text from the tag\n",
    "                tag_text = tag.get_text()\n",
    "                # Replace the tag with its text content\n",
    "                tag.replace_with(tag_text)\n",
    "                tag.text.strip()\n",
    "            a_tags_to_remove = article.find_all('a', class_ =\"TextlinkBaiviet\")\n",
    "            for a in a_tags_to_remove:\n",
    "                # Extract the text from the tag\n",
    "                tag_text = a .get_text()\n",
    "                # Replace the tag with its text content\n",
    "                a.replace_with(tag_text)\n",
    "                a.text.strip()\n",
    "            # set attribut for img\n",
    "            b = article.find_all('img')\n",
    "            for i in b:\n",
    "                if '.svg' in i['src']:\n",
    "                    i.decompose()\n",
    "            b = article.find_all('img')\n",
    "            for i in article.find_all('img'):\n",
    "                try:\n",
    "                    if 'imgFullMobile_chuan' in  i.parent['class'] or 'imgFullMobile' in  i.parent['class']:\n",
    "                        i.decompose()\n",
    "                except KeyError as e:\n",
    "                    print(e)   \n",
    "            for i in article.find_all('img'):\n",
    "                if 'data-original' in i.attrs.keys():\n",
    "                    i['src'] = i.get('data-original')\n",
    "                #i.attrs = ['class', 'alt', 'src', 'data-original']\n",
    "                #del i['onclick']\n",
    "                #del i['style']\n",
    "                #del i['class']\n",
    "                list_attr = ['src','alt','data-src']\n",
    "                for i in article.find_all('img'):\n",
    "                    for j in list(i.attrs.keys()):\n",
    "                        if j not in list_attr:\n",
    "                            i.attrs.pop(j)\n",
    "                for i in article.find_all('img'):\n",
    "                    if 'https://image-us.24h.com.vn/' not in i['src']:\n",
    "                        i['src']= i['data-src']\n",
    "            for script_or_style in article(['script', 'style']):\n",
    "                script_or_style.decompose()\n",
    "            for i in article.find_all(recursive = True):\n",
    "                try:\n",
    "                    del i['onclick']\n",
    "                    del i['id']\n",
    "                    del i['class']\n",
    "                    del i['style']\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "                    \n",
    "                except TypeError:\n",
    "                    continue\n",
    "            for i in article.find_all('img'):\n",
    "                i['class'] = \"aligncenter\"\n",
    "                i['width'] = 800\n",
    "                i['height'] = 400\n",
    "            source_tag = soup.new_tag('i') \n",
    "            source_tag.string = \"Nguồn: 24h.com.vn\"  # Set the content of <i> tag\n",
    "            #remove empty div or empty space from element\n",
    "            for i in article.find_all('div',recursive= True):\n",
    "                if i.string == None:\n",
    "                    if i.text =='':\n",
    "                        i.decompose()\n",
    "            for element in article.find_all(recursive = True,string=True):\n",
    "                if isinstance(element, NavigableString) and element.strip() == '':\n",
    "                    element.extract()\n",
    "            #remove html comment from element   \n",
    "            for comment in article.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "                comment.extract()\n",
    "            # Append the <i> tag as the last child of the <article> tag\n",
    "            article.append(source_tag)\n",
    "            \"\"\"a_tags_to_remove = article.find_all('a', class_ = \"TextlinkBaiviet\")\n",
    "            for a in a_tags_to_remove:\n",
    "                # Extract the text from the tag\n",
    "                tag_text = a .get_text()\n",
    "                # Replace the tag with its text content\n",
    "                a.replace_with(tag_text)\n",
    "                a.text.strip()\"\"\"\n",
    "            #Handling a tag \n",
    "            a_tags = soup.find_all('a')\n",
    "            for a_tag in a_tags:\n",
    "                # Check if the <a> tag has no child tags\n",
    "                if all(not isinstance(child, Tag) for child in a_tag.children):\n",
    "                    # Convert <a> tag to its text if it has no child tags\n",
    "                    a_tag.replace_with(a_tag.get_text())\n",
    "                else:\n",
    "                    # If <a> tag has child elements, replace it with a <span> tag but keep the children\n",
    "                    new_span = soup.new_tag(\"span\")\n",
    "                    new_span.extend(a_tag.contents)  # Use extend to add all child elements\n",
    "                    a_tag.replace_with(new_span)    \n",
    "            remove_div(article)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "    return article\n",
    "#convert time from post to the format of output\n",
    "def convert_string(input_str):\n",
    "    # Extract the date part using regular expression\n",
    "    pattern = r'\\d{2}/\\d{2}/\\d{4}'\n",
    "    date_part = re.search(pattern, input_str).group()\n",
    "    match = re.search(pattern, input_str)\n",
    "    if match:\n",
    "        # Parse the date string into a datetime object\n",
    "        date_obj = datetime.strptime(date_part, '%d/%m/%Y')\n",
    "        formatted_date = date_obj.strftime('%Y-%m-%d')\n",
    "        return formatted_date\n",
    "    else:\n",
    "        return \"\"\n",
    "#Convert time from post to datetime format\n",
    "def convert_time_string(posted_date):\n",
    "    pattern = r'\\d{2}/\\d{2}/\\d{4}'\n",
    "    match = re.search(pattern, posted_date)\n",
    "\n",
    "    if match:\n",
    "        date_string = match.group()\n",
    "\n",
    "        # Convert to datetime object\n",
    "        datetime_obj = datetime.strptime(date_string, \"%d/%m/%Y\")\n",
    "        return datetime_obj\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_post(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(response.content, 'html5lib')\n",
    "        content = get_content(url)\n",
    "        post_time = soup.find('time', class_ = 'cate-24h-foot-arti-deta-cre-post').text.strip()\n",
    "        published_date = convert_string(post_time)\n",
    "        title = soup.find('h1').text.strip()\n",
    "        #h2 = soup.find('h2').text.strip()\n",
    "        #images_src = [i.attrs['src'] for i in soup.find('article', class_= 'cate-24h-foot-arti-deta-info').find_all('img')[:-1] if 'svg' not in i.attrs['src']]\n",
    "        #images_src = [img['data-original'] if 'https://image-us.24h.com.vn' not in img['src'] else img['src'] if 'svg' not in img['src'] else '' for img in soup.find('article', class_ = 'cate-24h-foot-arti-deta-info').find_all('img')[:-1]]\n",
    "        #text_list = [ child.text for child in soup.find('article', class_= 'cate-24h-foot-arti-deta-info').find_all('p')[:-3] if re.sub(r'\\n+', '', child.text) != \"\"]\n",
    "        #text_list = [h2] + text_list\n",
    "        #return content,title,published_date\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            time.sleep(3)\n",
    "            soup = BeautifulSoup(response.content, 'html5lib')\n",
    "            content = get_content(url)\n",
    "            post_time = soup.find('div', class_ = 'magazine_event_date').text.strip()\n",
    "            published_date = convert_string(post_time)\n",
    "            title = soup.find('div', class_ = 'titZing').text.strip()\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            content = ''\n",
    "            title = ''\n",
    "            published_date= ''\n",
    "    return content,title,published_date\n",
    "\n",
    "\n",
    "def get_list_url(cat_url):\n",
    "    urls = []\n",
    "    response = requests.get(cat_url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(response.content,'html5lib')\n",
    "    try:\n",
    "        news_block  = soup.find('section', id = 'tin_bai_noi_bat_khac')\n",
    "        recent_news = soup.find('section', class_ = 'cate-24h-car-news-top').find_all('a')\n",
    "        for url in recent_news:\n",
    "            if 'https://www.24h.com.vn/' in url['href']:\n",
    "                if url['href'] not in urls:\n",
    "                    urls.append(url['href'])\n",
    "        url_list = news_block.find_all('a')\n",
    "        #print(len(url_list))\n",
    "        for url in url_list:\n",
    "            if 'https://www.24h.com.vn/' in url['href']:\n",
    "                if url['href'] not in urls:\n",
    "                    urls.append(url['href'])\n",
    "        print(len(urls))\n",
    "        return urls\n",
    "    except AttributeError:\n",
    "        news_block  = soup.find('div', class_ = 'cate-24h-foot-home-latest-list')\n",
    "        recent_news = soup.find('section', class_ = 'cate-24h-foot-box-news-hightl-cate-2 border-bottom no-padding margin-bottom-20').find_all('h3')\n",
    "    \n",
    "        for element in recent_news:\n",
    "            url = element.find('a')['href']\n",
    "            urls.append(url)\n",
    "        url_list = news_block.find_all('h3')\n",
    "        #print(len(url_list))\n",
    "        for element in url_list:\n",
    "            url = element.find('a')\n",
    "            if 'https://www.24h.com.vn/' in url['href']:\n",
    "                if url['href'] not in urls:\n",
    "                    urls.append(url['href'])\n",
    "        print(len(urls))\n",
    "        return urls\n",
    "def filter_list(urls):\n",
    "    filtered_urls = []\n",
    "    crawl_time = datetime.fromtimestamp(time.time() - 1*24*3600)\n",
    "    for i in urls:\n",
    "        response = requests.get(i)\n",
    "        soup = BeautifulSoup(response.content, 'html5lib')\n",
    "        try:\n",
    "            date_posted = soup.find('time',class_ ='cate-24h-foot-arti-deta-cre-post').text.strip()\n",
    "            date_posted_norm = convert_time_string(date_posted)\n",
    "            #if ( (date_posted_norm.day == crawl_time.day) and (date_posted_norm.month == crawl_time.month) and (date_posted_norm.year == crawl_time.year) ):\n",
    "            if date_posted_norm >= datetime.combine(crawl_time, datetime.min.time()):\n",
    "                filtered_urls.append(i)\n",
    "                print(i)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return filtered_urls\n",
    "def add_list(web_24h_com_vn):\n",
    "    for i in list(web_24h_com_vn['urls'].keys()):\n",
    "        for j in list(web_24h_com_vn['urls'][i]['sub-category'].keys()):\n",
    "            \n",
    "            urls = get_list_url(web_24h_com_vn['urls'][i]['sub-category'][j]['url'])\n",
    "            print(i,j,web_24h_com_vn['urls'][i]['sub-category'][j]['url'])\n",
    "            web_24h_com_vn['urls'][i]['sub-category'][j]['url_list'] = filter_list(urls)\n",
    "\n",
    "def add_post(web_24h_com_vn):\n",
    "    for i in list(web_24h_com_vn['urls'].keys()):\n",
    "        for j in list(web_24h_com_vn['urls'][i]['sub-category'].keys()):\n",
    "            web_24h_com_vn['urls'][i]['sub-category'][j]['content'] = {}\n",
    "            list_key = [v for v in range(0,len(web_24h_com_vn['urls'][i]['sub-category'][j]['url_list']))]\n",
    "            for u in list_key:\n",
    "                web_24h_com_vn['urls'][i]['sub-category'][j]['content'][u] = {}\n",
    "                if u != \"\":\n",
    "                    web_24h_com_vn['urls'][i]['sub-category'][j]['content'][u]['text'] ,web_24h_com_vn['urls'][i]['sub-category'][j]['content'][u]['title'],web_24h_com_vn['urls'][i]['sub-category'][j]['content'][u]['published_date'] = get_post(web_24h_com_vn['urls'][i]['sub-category'][j]['url_list'][u])\n",
    "                    print(i,j,web_24h_com_vn['urls'][i]['cate_id'],web_24h_com_vn['urls'][i]['sub-category'][j]['name'],web_24h_com_vn['urls'][i]['sub-category'][j]['name'],web_24h_com_vn['urls'][i]['sub-category'][j]['content'][u]['title'],web_24h_com_vn['urls'][i]['sub-category'][j]['url_list'][u])\n",
    "                \n",
    "\n",
    "def get_news():\n",
    "    web_24h_com_vn = {\n",
    "        \"home_page\":\"https://www.24h.com.vn/\",\n",
    "        \"urls\":{\n",
    "            \"tech\":\n",
    "            {\n",
    "             \"url\":\"https://www.24h.com.vn/cong-nghe-thong-tin-c55.html\",\n",
    "             \"cate_id\":57,\n",
    "             \"sub-category\":{\n",
    "                0:{\"name\":\"Game\",\n",
    "                 \"url\":\"https://www.24h.com.vn/game-c69.html\"},\n",
    "                1:{\"name\":\"Phần mềm\",\n",
    "                 \"url\":\"https://www.24h.com.vn/phan-mem-ngoai-c302.html\"},\n",
    "                2:{\"name\":\"Khoa học\",\n",
    "                 \"url\":\"https://www.24h.com.vn/khoa-hoc-c782.html\"},\n",
    "                3:{\"name\":\"Mạng xã hội\",\n",
    "                 \"url\":\"https://www.24h.com.vn/mang-xa-hoi-c889.html\"},\n",
    "                4:{\"name\":\"Thủ thuật - Tiện ích\"\n",
    "                 ,\"url\":\"https://www.24h.com.vn/thu-thuat-tien-ich-c68.html\"},\n",
    "                5:{\"name\":\"Sợ Virus\",\n",
    "                 \"url\":\"https://www.24h.com.vn/tim-hieu-virus-c57.html\"},\n",
    "                6:{\"name\":\"Máy in/phụ kiện\",\n",
    "                 \"url\":\"https://www.24h.com.vn/may-in/phu-kien-c291.html\"},\n",
    "                7:{\"name\":\"Khám phá công nghệ\",\n",
    "                 \"url\":\"https://www.24h.com.vn/kham-pha-cong-nghe-c675.html\"}\n",
    "             }\n",
    "            }\n",
    "            ,\n",
    "            \"youths\":\n",
    "            {\n",
    "            \"url\":\"https://www.24h.com.vn/ban-tre-cuoc-song-c64.html\",\n",
    "            \"cate_id\":60,\n",
    "             \"sub-category\":{\n",
    "                0:{\"name\":\"Chuyện công sở\",\"url\":\"https://www.24h.com.vn/chuyen-cong-so-c180.html\"},\n",
    "                1:{\"name\":\"Tình yêu - Giới Tính\",\"url\":\"https://www.24h.com.vn/tinh-yeu-gioi-tinh-c306.html\"},\n",
    "                2:{\"name\":\"Ngoại tình\",\"url\":\"https://www.24h.com.vn/ngoai-tinh-c435.html\"},\n",
    "                3:{\"name\":\"Giới trẻ\",\"url\":\"https://www.24h.com.vn/gioi-tre-c434.html\"},\n",
    "                4:{\"name\":\"Hotgirl - Hotboy\",\"url\":\"https://www.24h.com.vn/hotgirl-hot-boy-c64e3398.html\"},\n",
    "                5:{\"name\":\"Nhịp sống trẻ\",\"url\":\"https://www.24h.com.vn/nhip-song-tre-c685.html\"}\n",
    "             }\n",
    "            }\n",
    "            ,\n",
    "            \"showbiz\":\n",
    "            {\n",
    "            \"cate_id\":59,\n",
    "            \"url\":\"https://www.24h.com.vn/doi-song-showbiz-c729.html\",\n",
    "             \"sub-category\":{\n",
    "                0:{\"name\":\"Sao Việt\",\"url\":\"https://www.24h.com.vn/sao-viet-c757.html\"},\n",
    "                1:{\"name\":\"24h gặp gỡ\",\"url\":\"https://www.24h.com.vn/gap-go-24h-c729e6820.html\"},\n",
    "                2:{\"name\":\"Talk với sao\",\"url\":\"https://www.24h.com.vn/doi-thoai-cung-sao-c730.html\"},\n",
    "                3:{\"name\":\"Sao châu Á\",\"url\":\"https://www.24h.com.vn/sao-chau-a-c759.html\"},\n",
    "            }\n",
    "            },\n",
    "            \"cars\":\n",
    "            {\n",
    "            \"cate_id\":58,\n",
    "            \"url\":\"https://www.24h.com.vn/o-to-c747.html\",\n",
    "             \"sub-category\":{\n",
    "                0:{\"name\":\"Tin tức ô tô\",\"url\":\"https://www.24h.com.vn/tin-tuc-o-to-c332.html\"},\n",
    "                1:{\"name\":\"Bảng giá xe ô tô\",\"url\":\"https://www.24h.com.vn/bang-gia-xe-o-to-c807.html\"},\n",
    "                2:{\"name\":\"Tư vấn\",\"url\":\"https://www.24h.com.vn/tu-van-c240.html\"},\n",
    "                3:{\"name\":\"Ngắm xe\",\"url\":\"https://www.24h.com.vn/anh-nguoi-dep-va-xe-c199.html\"},\n",
    "                4:{\"name\":\"Đánh giá xe\",\"url\":\"https://www.24h.com.vn/so-sanh-xe-c805.html\"},\n",
    "            }\n",
    "            },\n",
    "            \n",
    "            \"người đẹp\":\n",
    "            {\n",
    "            \"cate_id\":63,\n",
    "            \"url\":\"https://www.24h.com.vn/hau-truong-ngoi-sao-the-thao-c797.html#\",\n",
    "             \"sub-category\":{\n",
    "                0:{\"name\":\"người đẹp thể thao\",\"url\":\"https://www.24h.com.vn/hau-truong-ngoi-sao-the-thao-c797.html\"}\n",
    "            }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "#\n",
    "    add_list(web_24h_com_vn)\n",
    "    add_post(web_24h_com_vn)\n",
    "    return web_24h_com_vn\n",
    "def send_post_to_5goals(title,content,category_id,published_date):\n",
    "    # URL of the API endpoint (this is a placeholder and needs to be replaced with the actual URL)\n",
    "    url = \"https://api2023.5goal.com/wp-json/custom/createPost\"\n",
    "    \n",
    "    # Data to be sent in the POST request\n",
    "    data = {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"category_id\": category_id,\n",
    "        \"token\": '5goalvodichcmnl',  # Replace with your actual access token\n",
    "        \"published_date\": published_date\n",
    "          # Replace with the actual category ID as required\n",
    "    }\n",
    "    \n",
    "    # Sending the POST request\n",
    "    response = requests.post(url, data=data)\n",
    "    \n",
    "    # Checking the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"The post was successfully created.\")\n",
    "        print(\"Response:\", response.text)  # Prints the response text from the server\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to create the post. Status code: {response.status_code}\")\n",
    "def main():\n",
    "    web_24h_com_vn = get_news()\n",
    "    for i in list(web_24h_com_vn['urls'].keys()):\n",
    "    #web_24h_com_vn2['url'][i]['cate_id']\n",
    "        for j in list(web_24h_com_vn['urls'][i]['sub-category']):\n",
    "            url_list =  web_24h_com_vn['urls'][i]['sub-category'][j]['url_list']\n",
    "            for t in range(0,len(url_list)):\n",
    "                text = web_24h_com_vn['urls'][i]['sub-category'][j]['content'][t]['text']\n",
    "                title = web_24h_com_vn['urls'][i]['sub-category'][j]['content'][t]['title']\n",
    "                published_date = web_24h_com_vn['urls'][i]['sub-category'][j]['content'][t]['published_date']\n",
    "                cate_id = web_24h_com_vn['urls'][i]['cate_id']\n",
    "                print(\"title: \", title, \"\\n\")\n",
    "                print(\"date: \", published_date, \"\\n\")\n",
    "                print(\"id: \", cate_id, \"\\n\")\n",
    "                try:\n",
    "                    text_len = len(text.text)\n",
    "                    if text_len <500:\n",
    "                        print(text.text)\n",
    "                        continue\n",
    "                    if \"Clip:\" in title:\n",
    "                        print(title)\n",
    "                        continue\n",
    "                    else:\n",
    "                         send_post_to_5goals(title,str(text),cate_id,published_date)\n",
    "                except (AttributeError,TypeError):\n",
    "                    continue\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "39aa156b-832d-42b8-a658-e61404a8942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_url(cat_url):\n",
    "    urls = []\n",
    "    response = requests.get(cat_url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(response.content,'html5lib')\n",
    "    try:\n",
    "        try:\n",
    "            news_block  = soup.find('section', id = 'tin_bai_noi_bat_khac').find_all('a')\n",
    "            #print(len(url_list))\n",
    "            for url in news_block:\n",
    "                if 'https://www.24h.com.vn/' in url['href']:\n",
    "                    if url['href'] not in urls:\n",
    "                        urls.append(url['href'])\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            recent_news = soup.find('section', class_ = 'cate-24h-car-news-top').find_all('a')\n",
    "            for url in recent_news:\n",
    "                if 'https://www.24h.com.vn/' in url['href']:\n",
    "                    if url['href'] not in urls:\n",
    "                        urls.append(url['href'])\n",
    "        except AttributeError as e:\n",
    "            print(e)             \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            news_block  = soup.find('section', class_ = 'cate-24h-foot-home-2-col margin-top-20')\n",
    "            url_list = news_block.find_all('h3')\n",
    "            #print(len(url_list))\n",
    "            for element in url_list:\n",
    "                \n",
    "                url = element.find('a')\n",
    "                if 'https://www.24h.com.vn/' in url['href']:\n",
    "                    if url['href'] not in urls:\n",
    "                        urls.append(url['href'])\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            recent_news = soup.find('section', class_ = 'cate-24h-foot-box-news-hightl-cate-2 border-bottom no-padding margin-bottom-20').find_all('h3')\n",
    "            for element in recent_news:\n",
    "                url = element.find('a')['href']\n",
    "                urls.append(url)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            recent_news2  = soup.find('section', class_ = 'section-tin-noi-bat')\n",
    "            url_list = recent_news2.find_all('h3')\n",
    "            #print(len(url_list))\n",
    "            for element in url_list:\n",
    "                url = element.find('a')\n",
    "                if 'https://www.24h.com.vn/' in url['href']:\n",
    "                    if url['href'] not in urls:\n",
    "                        urls.append(url['href'])\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "        print(len(urls))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2517a2d8-1791-48b1-8bfc-abac8d0c5640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'find_all'\n",
      "'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "test = get_list_url('https://www.24h.com.vn/sao-chau-a-c759.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0262d3c7-4fe5-4626-a6c7-fd63af65e2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03529f0f-3f54-4b5b-a625-5a08aa963afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9652c043-f4e0-466b-85ef-2ae5f635eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.24h.com.vn/the-thao/cuu-so-1-the-gioi-naomi-osaka-nuoi-giac-mong-lon-c101a1525872.html')\n",
    "soup = BeautifulSoup(response.content, 'html5lib')\n",
    "date_posted = soup.find('time',class_ ='cate-24h-foot-arti-deta-cre-post').text.strip()\n",
    "date_posted_norm = convert_time_string(date_posted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec29738b-40a9-4a1d-9933-2e2a60887fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 12, 10, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_posted_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa905b8-fcb2-499a-b8d6-77eed24a0fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 12, 13, 0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_time = datetime.fromtimestamp(time.time() - 1*24*3600)\n",
    "\n",
    "datetime.combine(crawl_time, datetime.min.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e8c5b-4707-4c87-968d-a4ad7802a5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
