{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eb830eea-57ed-4e6d-8151-83a5ceec8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup,NavigableString, Comment\n",
    "import lxml\n",
    "def convert_string(input_str):\n",
    "    # Regular expression pattern to match a date in the format dd-mm-yyyy\n",
    "    pattern =r'\\d+/\\d+/\\d+'\n",
    "    \n",
    "    # Search for the pattern in the string\n",
    "    match = re.search(pattern, input_str)\n",
    "    if match:\n",
    "    # Extract and print the date if found\n",
    "        date_str = match.group(0)\n",
    "        date_obj = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "        date_str = date_obj.strftime('%Y-%m-%d')\n",
    "        return date_str,date_obj\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_datetime_obj(string):\n",
    "\n",
    "    # Regular expression pattern to match the date\n",
    "    pattern = r'\\d{2}/\\d{2}/\\d{4}'\n",
    "    \n",
    "    # Search for the pattern in the string and extract the date\n",
    "    match = re.search(pattern, string)\n",
    "    if match:\n",
    "        date_str = match.group(0)\n",
    "        # Convert the extracted date string to datetime object\n",
    "        date_obj = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "        date_str = date_obj.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        date_obj = None\n",
    "        date_str = None\n",
    "    \n",
    "    return date_obj,date_str\n",
    "def get_content_vtv(url):\n",
    "    response = requests.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1').text.strip()\n",
    "    date = soup.find('p', class_ = 'news-info').text.strip()\n",
    "    published_date = get_datetime_obj(date)[1]\n",
    "    first_paragraph = soup.find('h2', class_  = 'sapo')\n",
    "    article = soup.find('div', id = 'entry-body')\n",
    "    #strong_tag = soup.new_tag('strong') \n",
    "    #strong_tag.string = first_paragraph.text # Set the content of <i> tag\n",
    "    # Insert the new element as the first child\n",
    "    article.insert(0, first_paragraph)\n",
    "    \n",
    "    for script_or_style in article(['script', 'style','iframe','video',]):\n",
    "                script_or_style.decompose()\n",
    "    \n",
    "    caption_text_list = article.find_all('div', class_ ='PhotoCMS_Caption')\n",
    "    tags_to_remove = article.find_all(['a', 'span'])\n",
    "    for tag in tags_to_remove:\n",
    "        # Extract the text from the tag\n",
    "        tag_text = tag.get_text()\n",
    "        # Replace the tag with its text content\n",
    "        tag.replace_with(tag_text)\n",
    "        tag.text.strip()\n",
    "    #remove all image attributes except somes from list\n",
    "    list_attr = ['src','alt','data-original']\n",
    "    for i in article.find_all('img'):\n",
    "        for j in list(i.attrs.keys()):\n",
    "            if j not in list_attr:\n",
    "                i.attrs.pop(j)\n",
    "    img_list = article.find_all('img')\n",
    "    n_img = len(img_list)\n",
    "    for i in range(0,n_img):\n",
    "           \n",
    "        caption_start = NavigableString(\"[caption id=\\\"\\\" align=\\\"aligncenter\\\" width=\\\"800\\\"]\")\n",
    "        try:\n",
    "            caption_text = NavigableString(caption_text_list[i].find('p').get_text())\n",
    "        except (IndexError,AttributeError):\n",
    "            continue\n",
    "        caption_end = NavigableString(\"[/caption]\")\n",
    "        # Insert the custom tags and caption text around the <img> tag\n",
    "        img_list[i].insert_before(caption_start)\n",
    "        img_list[i].insert_after(caption_end)\n",
    "        img_list[i].insert_after(caption_text) \n",
    "    for i in article.find_all('img'):\n",
    "        try:\n",
    "            i['src'] = i['data-src']\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for i in caption_text_list:\n",
    "        i.decompose()\n",
    "    # add avatar photo on top\n",
    "    first_photo = soup.find('img', class_ = 'news-avatar')\n",
    "    article.insert(0, first_photo)\n",
    "    # centering the image\n",
    "    for i in article.find_all('img'):\n",
    "        i['class'] = \"aligncenter\"\n",
    "        #i['width'] = 800\n",
    "       # i['height'] = 400\n",
    "        del i['data-src']\n",
    "    list_attr = ['src','alt','data-ogiginal','class']\n",
    "    for i in article.find_all(recursive = True):\n",
    "        for j in list(i.attrs.keys()):\n",
    "            if j not in list_attr:\n",
    "                i.attrs.pop(j)\n",
    "    #remove empty div or empty space from element\n",
    "    for item in article.find_all('div'):\n",
    "        if item.string ==\"\":\n",
    "            item.decompose()\n",
    "    for element in article.find_all(recursive = True,string=True):\n",
    "        if isinstance(element, NavigableString) and element.strip() == '':\n",
    "            element.extract()\n",
    "    #remove html comment from element   \n",
    "    article.find_all('p',recursive = True)[-1].decompose()\n",
    "    for comment in article.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "    \n",
    "    #article.find_all(recursive = True)[-1].decompose()\n",
    "    #article.find('div', class_ ='width_common box-tinlienquanv2').decompose()\n",
    "    source_tag = soup.new_tag('i') \n",
    "    source_tag.string = \"Nguồn: vtv.vn\"  # Set the content of <i> tag\n",
    "    article.append(source_tag)\n",
    "    return article, title, published_date\n",
    "def get_post(url):\n",
    "    try:\n",
    "        content,title,published_date = get_content_vtv(url)\n",
    "        return content,title,published_date\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "def get_list_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    items = soup.find_all('h3')\n",
    "    urls = []\n",
    "    for i in items:\n",
    "        path = i.find('a')['href']\n",
    "        url = 'https://vtv.vn/'+ path\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "def filter_list(urls):\n",
    "    filtered_urls = []\n",
    "    crawl_time = datetime.fromtimestamp(time.time()-0*24*3600)\n",
    "    for i in range(0,len(urls)):\n",
    "        try:\n",
    "            published_date = get_content_vtv(urls[i])\n",
    "            date_posted_norm = get_datetime_obj(published_date)[2]\n",
    "            if ( (date_posted_norm.day == crawl_time.day) and (date_posted_norm.month == crawl_time.month) and (date_posted_norm.year == crawl_time.year) ):\n",
    "                filtered_urls.append(urls[i])\n",
    "                #print(i)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return filtered_urls\n",
    "#add list url to json\n",
    "#add list url to json\n",
    "def add_list(web_json_obj):\n",
    "    for i in list(web_json_obj['urls'].keys()):\n",
    "        for j in list(web_json_obj['urls'][i]['sub-category'].keys()):  \n",
    "            urls = get_list_url(web_json_obj['urls'][i]['sub-category'][j]['url'])\n",
    "            print(i,j,web_json_obj['urls'][i]['sub-category'][j]['url'])\n",
    "            web_json_obj['urls'][i]['sub-category'][j]['url_list'] = filter_list(urls)\n",
    "# add post content from get content function to json object\n",
    "# add post content from get content function to json object\n",
    "def add_post(web_json_obj):\n",
    "    for i in list(web_json_obj['urls'].keys()):\n",
    "        for j in list(web_json_obj['urls'][i]['sub-category'].keys()):\n",
    "            web_json_obj['urls'][i]['sub-category'][j]['content'] = {}\n",
    "            list_key = [v for v in range(0,len(web_json_obj['urls'][i]['sub-category'][j]['url_list']))]\n",
    "            for u in list_key:\n",
    "                web_json_obj['urls'][i]['sub-category'][j]['content'][u] = {}\n",
    "                if u != \"\":\n",
    "                    web_json_obj['urls'][i]['sub-category'][j]['content'][u]['text'] ,web_json_obj['urls'][i]['sub-category'][j]['content'][u]['title'],web_json_obj['urls'][i]['sub-category'][j]['content'][u]['published_date'] = get_post(web_json_obj['urls'][i]['sub-category'][j]['url_list'][u])\n",
    "                    print(i,j,web_json_obj['urls'][i]['sub-category'][j]['cate_id'],web_json_obj['urls'][i]['sub-category'][j]['name'],web_json_obj['urls'][i]['sub-category'][j]['name'],web_json_obj['urls'][i]['sub-category'][j]['content'][u]['title'],web_json_obj['urls'][i]['sub-category'][j]['url_list'][u])\n",
    "#add all necessary information to json object\n",
    "def get_news_vtv():\n",
    "    _vtv= {\n",
    "            \"home_page\":\"https://vtv.vn/\",\n",
    "            \"urls\":{\n",
    "                \"Quần vọt\":\n",
    "                {\n",
    "                 \"url\":\"https://vtv.vn/the-thao/tennis.htm#\",\n",
    "                 \"sub-category\":{  \n",
    "                    0:{\"name\":\"Tennis\",\n",
    "                     \"url\":\"https://vtv.vn/the-thao/tennis.htm\",\n",
    "                     \"cate_id\":57,\n",
    "                      \"url_list\" : []},\n",
    "                 }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "#\n",
    "    add_list(_vtv)\n",
    "    add_post(_vtv)\n",
    "    return _vtv\n",
    "#send post content to wordpress via endpoint\n",
    "def send_post_to_5goals(title,content,category_id,published_date):\n",
    "    # URL of the API endpoint (this is a placeholder and needs to be replaced with the actual URL)\n",
    "    url = \"https://api2023.5goal.com/wp-json/custom/createPost\"\n",
    "    \n",
    "    # Data to be sent in the POST request\n",
    "    data = {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"category_id\": category_id,\n",
    "        \"token\": 'draftpost',#'5goalvodichcmnl',  # Replace with your actual access token\n",
    "        \"published_date\": published_date,\n",
    "        \"domain\":\"vtv\"\n",
    "          # Replace with the actual category ID as required\n",
    "    }\n",
    "    # Sending the POST request\n",
    "    response = requests.post(url, data=data)\n",
    "    \n",
    "    # Checking the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"The post was successfully created.\")\n",
    "        print(\"Response:\", response.text)  # Prints the response text from the server\n",
    "    else:\n",
    "        print(f\"Failed to create the post. Status code: {response.status_code}\")\n",
    "def main():\n",
    "    _vtv = get_news_vtv()\n",
    "    for i in [list(_vtv['urls'].keys())[0]]:\n",
    "    #web_24h_com_vn2['url'][i]['cate_id']\n",
    "        for j in list(_vtv['urls'][i]['sub-category']):\n",
    "            url_list =  _vtv['urls'][i]['sub-category'][j]['url_list']\n",
    "            print(url_list)\n",
    "            for t in range(0,len(url_list)):\n",
    "                content = _vtv['urls'][i]['sub-category'][j]['content'][t]['text']\n",
    "                title = _vtv['urls'][i]['sub-category'][j]['content'][t]['title']\n",
    "                published_date = _vtv['urls'][i]['sub-category'][j]['content'][t]['published_date']\n",
    "                cate_id = _vtv['urls'][i]['sub-category'][j]['cate_id']\n",
    "                print(title, url_list[t],content)\n",
    "                #send_post_to_5goals(title,str(content), cate_id, published_date)\n",
    "                time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a71400-945b-45e4-ba2e-c8d1795f27df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f86a3bb0-38f0-45b3-8ed2-d1d917c0739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quần vọt 0 https://vtv.vn/the-thao/tennis.htm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _vtv \u001b[38;5;241m=\u001b[39m get_news_vtv()\n",
      "Cell \u001b[1;32mIn[101], line 191\u001b[0m, in \u001b[0;36mget_news_vtv\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m     _vtv\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    176\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_page\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://vtv.vn/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    177\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m             }\n\u001b[0;32m    189\u001b[0m         }\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     add_list(_vtv)\n\u001b[0;32m    192\u001b[0m     add_post(_vtv)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vtv\n",
      "Cell \u001b[1;32mIn[101], line 160\u001b[0m, in \u001b[0;36madd_list\u001b[1;34m(web_json_obj)\u001b[0m\n\u001b[0;32m    158\u001b[0m urls \u001b[38;5;241m=\u001b[39m get_list_url(web_json_obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-category\u001b[39m\u001b[38;5;124m'\u001b[39m][j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(i,j,web_json_obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-category\u001b[39m\u001b[38;5;124m'\u001b[39m][j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 160\u001b[0m web_json_obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murls\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-category\u001b[39m\u001b[38;5;124m'\u001b[39m][j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filter_list(urls)\n",
      "Cell \u001b[1;32mIn[101], line 145\u001b[0m, in \u001b[0;36mfilter_list\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     published_date \u001b[38;5;241m=\u001b[39m get_content_vtv(urls[i])\n\u001b[1;32m--> 145\u001b[0m     date_posted_norm \u001b[38;5;241m=\u001b[39m get_datetime_obj(published_date)[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ( (date_posted_norm\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m==\u001b[39m crawl_time\u001b[38;5;241m.\u001b[39mday) \u001b[38;5;129;01mand\u001b[39;00m (date_posted_norm\u001b[38;5;241m.\u001b[39mmonth \u001b[38;5;241m==\u001b[39m crawl_time\u001b[38;5;241m.\u001b[39mmonth) \u001b[38;5;129;01mand\u001b[39;00m (date_posted_norm\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m==\u001b[39m crawl_time\u001b[38;5;241m.\u001b[39myear) ):\n\u001b[0;32m    147\u001b[0m         filtered_urls\u001b[38;5;241m.\u001b[39mappend(urls[i])\n",
      "Cell \u001b[1;32mIn[101], line 28\u001b[0m, in \u001b[0;36mget_datetime_obj\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     25\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Search for the pattern in the string and extract the date\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(pattern, string)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[0;32m     30\u001b[0m     date_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:176\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msearch(string)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'tuple'"
     ]
    }
   ],
   "source": [
    "_vtv = get_news_vtv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c789df25-b628-4e82-a888-187147722405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home_page': 'https://vtv.vn/',\n",
       " 'urls': {'Quần vọt': {'url': 'https://vtv.vn/the-thao/tennis.htm#',\n",
       "   'sub-category': {0: {'name': 'Tennis',\n",
       "     'url': 'https://vtv.vn/the-thao/tennis.htm',\n",
       "     'cate_id': 57,\n",
       "     'url_list': [],\n",
       "     'content': {}}}}}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_vtv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "802eb626-d783-44cd-9571-b4884006befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_list_url('https://vtv.vn/the-thao/tennis.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1267bdc8-cf69-4f60-8479-ca5fb5a0bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_content_vtv('https://vtv.vn/tennis/su-phat-trien-cua-cac-tay-vot-vo-dich-next-gen-atp-finals-20231204101144735.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6255b04-3538-4d27-ba94-8286975628e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_datetime_obj('https://vtv.vn//tennis/rafael-nadal-va-nhung-ky-vong-cho-su-tro-lai-2023120410170502.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "20c1b26a-5f0f-4883-800b-62f63cdd285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list(urls):\n",
    "    filtered_urls = []\n",
    "    crawl_time = datetime.fromtimestamp(time.time()-0*24*3600)\n",
    "    for i in range(0,len(urls)):\n",
    "        try:\n",
    "            published_date = get_content_vtv(urls[i])[2]\n",
    "            date_posted_norm = datetime.strptime(published_date,'%Y-%m-%d')\n",
    "            if ( (date_posted_norm.day == crawl_time.day) and (date_posted_norm.month == crawl_time.month) and (date_posted_norm.year == crawl_time.year) ):\n",
    "                filtered_urls.append(urls[i])\n",
    "                #print(i)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return filtered_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c01e8da6-0061-4ca0-9c6a-9d15b03e6d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidURL",
     "evalue": "Invalid URL 'https:/vtv.vn//tennis/rafael-nadal-va-nhung-ky-vong-cho-su-tro-lai-2023120410170502.htm': No host supplied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_content_vtv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps:/vtv.vn//tennis/rafael-nadal-va-nhung-ky-vong-cho-su-tro-lai-2023120410170502.htm\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[1;32mIn[101], line 40\u001b[0m, in \u001b[0;36mget_content_vtv\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_content_vtv\u001b[39m(url):\n\u001b[1;32m---> 40\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     41\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     42\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:486\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    483\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    485\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 486\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    487\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    488\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    489\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    490\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    491\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[0;32m    492\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[0;32m    493\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[0;32m    494\u001b[0m     ),\n\u001b[0;32m    495\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[0;32m    496\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[0;32m    497\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[0;32m    498\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    499\u001b[0m )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:445\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# In general, we want to try IDNA encoding the hostname if the string contains\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# non-ASCII characters. This allows users to automatically get the correct IDNA\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# behaviour. For strings containing only ASCII characters, we need to also verify\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# it doesn't start with a wildcard (*), before allowing the unencoded hostname.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unicode_is_ascii(host):\n",
      "\u001b[1;31mInvalidURL\u001b[0m: Invalid URL 'https:/vtv.vn//tennis/rafael-nadal-va-nhung-ky-vong-cho-su-tro-lai-2023120410170502.htm': No host supplied"
     ]
    }
   ],
   "source": [
    "get_content_vtv('https:/vtv.vn//tennis/rafael-nadal-va-nhung-ky-vong-cho-su-tro-lai-2023120410170502.htm')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3809aa53-302f-4ff3-827a-0d186d94f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_datetime_obj('2023-12-04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d44df42-6f2a-4f87-a019-1d674c702e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://vtv.vn/tennis/rafael-nadal-va-nhung-ky-vong-cho-su-tro-lai-2023120410170502.htm')\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "title = soup.find('h1').text.strip()\n",
    "date = soup.find('p', class_ = 'news-info').text.strip()\n",
    "published_date = get_datetime_obj(date)[1]\n",
    "first_paragraph = soup.find('h2', class_  = 'sapo')\n",
    "article = soup.find('div', id = 'entry-body')\n",
    "#strong_tag = soup.new_tag('strong') \n",
    "#strong_tag.string = first_paragraph.text # Set the content of <i> tag\n",
    "# Insert the new element as the first child\n",
    "article.insert(0, first_paragraph)\n",
    "\n",
    "for script_or_style in article(['script', 'style','iframe','video',]):\n",
    "            script_or_style.decompose()\n",
    "\n",
    "caption_text_list = article.find_all('div', class_ ='PhotoCMS_Caption')\n",
    "tags_to_remove = article.find_all(['a', 'span'])\n",
    "for tag in tags_to_remove:\n",
    "    # Extract the text from the tag\n",
    "    tag_text = tag.get_text()\n",
    "    # Replace the tag with its text content\n",
    "    tag.replace_with(tag_text)\n",
    "    tag.text.strip()\n",
    "#remove all image attributes except somes from list\n",
    "list_attr = ['src','alt','data-original']\n",
    "for i in article.find_all('img'):\n",
    "    for j in list(i.attrs.keys()):\n",
    "        if j not in list_attr:\n",
    "            i.attrs.pop(j)\n",
    "img_list = article.find_all('img')\n",
    "n_img = len(img_list)\n",
    "for i in range(0,n_img):\n",
    "       \n",
    "    caption_start = NavigableString(\"[caption id=\\\"\\\" align=\\\"aligncenter\\\" width=\\\"800\\\"]\")\n",
    "    try:\n",
    "        caption_text = NavigableString(caption_text_list[i].find('p').get_text())\n",
    "    except (IndexError,AttributeError):\n",
    "        continue\n",
    "    caption_end = NavigableString(\"[/caption]\")\n",
    "    # Insert the custom tags and caption text around the <img> tag\n",
    "    img_list[i].insert_before(caption_start)\n",
    "    img_list[i].insert_after(caption_end)\n",
    "    img_list[i].insert_after(caption_text) \n",
    "for i in article.find_all('img'):\n",
    "    try:\n",
    "        i['src'] = i['data-src']\n",
    "    except KeyError:\n",
    "        continue\n",
    "for i in caption_text_list:\n",
    "    i.decompose()\n",
    "# add avatar photo on top\n",
    "first_photo = soup.find('img', class_ = 'news-avatar')\n",
    "article.insert(0, first_photo)\n",
    "# centering the image\n",
    "for i in article.find_all('img'):\n",
    "    i['class'] = \"aligncenter\"\n",
    "    #i['width'] = 800\n",
    "   # i['height'] = 400\n",
    "    del i['data-src']\n",
    "list_attr = ['src','alt','data-ogiginal','class']\n",
    "for i in article.find_all(recursive = True):\n",
    "    for j in list(i.attrs.keys()):\n",
    "        if j not in list_attr:\n",
    "            i.attrs.pop(j)\n",
    "#remove empty div or empty space from element\n",
    "for item in article.find_all('div'):\n",
    "    if item.string ==\"\":\n",
    "        item.decompose()\n",
    "for element in article.find_all(recursive = True,string=True):\n",
    "    if isinstance(element, NavigableString) and element.strip() == '':\n",
    "        element.extract()\n",
    "#remove html comment from element   \n",
    "article.find_all('p',recursive = True)[-1].decompose()\n",
    "for comment in article.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "    comment.extract()\n",
    "\n",
    "#article.find_all(recursive = True)[-1].decompose()\n",
    "#article.find('div', class_ ='width_common box-tinlienquanv2').decompose()\n",
    "source_tag = soup.new_tag('i') \n",
    "source_tag.string = \"Nguồn: vtv.vn\"  # Set the content of <i> tag\n",
    "article.append(source_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f282c683-942f-45d5-a079-17b0d711faad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content_detail ta-justify\" id=\"entry-body\"><img alt=\"\" class=\"aligncenter\" src=\"https://vtv1.mediacdn.vn/thumb_w/650/562122370168008704/2023/12/4/photo1701659799800-17016597999901359880287.jpg\"/><h2 class=\"sapo\">\n",
       "                        VTV.vn - Sự kiện quần vợt được nhắc tới nhiều trong tuần qua chính là thông báo về sự trở lại của Rafael Nadal. Cựu số 1 thế giới đã nhận được nhiều kỳ vọng với kế hoạch tương lai.\n",
       "                    </h2><p>Rafael Nadal xác nhận sẽ thi đấu ở giải quần vợt Brisbane mở rộng từ 31/12/2023 đến 7/1/2024. Đây là giải đấu đầu tiên của anh kể từ sau khi dừng bước tại vòng 2 Australia mở rộng 2023 do chấn thương cơ thắt lưng.<br/></p><p>Việc Rafael Nadal trở lại cũng nhận được sự quan tâm từ các đồng nghiệp. Tay vợt Taylor Fritz cho rằng: \"Tôi nghĩ giờ chưa phải là lúc anh ấy nghĩ tới việc giải nghệ. Nadal vẫn còn thời gian thi đấu thêm nhiều năm và việc anh ấy trở lại rõ ràng là tin tốt cho mọi người. Tôi không nghĩ quá nhiều nếu phải so tài với anh ấy. Có lẽ sẽ là 1 trận đấu không dễ dàng. Anh ấy luôn là vậy, rất mạnh mẽ trong mọi thời điểm và không dễ bỏ cuộc\".</p><div class=\"VCSortableInPreviewMode active noCaption\"><div><img alt=\"Rafael Nadal và những kỳ vọng cho sự trở lại - Ảnh 1.\" class=\"aligncenter\" src=\"https://vtv1.mediacdn.vn/thumb_w/640/562122370168008704/2023/12/4/photo-1-17016597908321283545801.jpg\"/></div></div><p>Nếu Rafael Nadal cảm thấy ổn, anh sẽ tiếp tục đăng ký thi đấu tại Australia mở rộng và gần như chắc chắn sẽ không phải thi đấu từ vòng loại, mà sẽ nhận được suất đặc cách. Ở tuổi 37, tay vợt người Tây Ban Nha vẫn còn những mục tiêu cụ thể, ngoài việc tham dự các giải Grand Slam. Đó là thêm 1 lần nữa góp mặt tại đấu trường Olympic. Năm tới, Olympic sẽ diễn ra tại Paris và môn quần vợt sẽ thi đấu trên mặt sân đất nện sở trường của anh - nơi tay vợt người Tây Ban Nha chắc chắn sẽ nhận được rất nhiều sự cổ vũ từ khán giả.</p><i>Nguồn: vtv.vn</i></div>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9bc24-cfca-413a-bf9b-94ef27aacf36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
