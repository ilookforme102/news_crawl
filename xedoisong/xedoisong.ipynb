{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3221d74-9721-4fb0-89e2-1ac1ac346ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup,NavigableString, Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2d6509-e9dd-4050-9a0b-201d685f6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_string(input_str):\n",
    "    # Regular expression pattern to match a date in the format dd-mm-yyyy\n",
    "    pattern = r'\\b\\d{2}/\\d{2}/\\d{4}\\b'\n",
    "    \n",
    "    # Search for the pattern in the string\n",
    "    match = re.search(pattern, input_str)\n",
    "    if match:\n",
    "    # Extract and print the date if found\n",
    "        date_str = match.group(0)\n",
    "        date_obj = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "        date =  date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "        return date\n",
    "    else:\n",
    "        'No date found'    \n",
    "#convert time string to datetime object\n",
    "def convert_time_string(posted_date):\n",
    "    # Regular expression pattern to match a date in the format dd-mm-yyyy\n",
    "    pattern = r'\\b\\d{2}/\\d{2}/\\d{4}\\b'\n",
    "\n",
    "    # Search for the pattern in the string and extract the date\n",
    "    match = re.search(pattern, posted_date)\n",
    "    if match:\n",
    "        date_str = match.group(0)\n",
    "        # Convert the extracted date string to datetime object\n",
    "        date_obj = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "        return date_obj\n",
    "    else:\n",
    "        return None \n",
    "# bongda24h has 2 type of time display, depends on type so we use the correct format for time converting\n",
    "def get_time_string(date_str):\n",
    "    if len(date_str)<=5:\n",
    "        crawl_time = datetime.fromtimestamp(time.time())\n",
    "        year = str(crawl_time.year)\n",
    "        date_str = date_str+\"/\"+year\n",
    "        date_obj = datetime.strptime(date_str, '%d/%m/%Y')\n",
    "        date =  date_obj.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        date = convert_string(date_str)\n",
    "        date_obj = datetime.strptime(date,'%Y-%m-%d')\n",
    "    return date_obj, date\n",
    "\n",
    "def get_content_autodaily(url):\n",
    "    # def get_content_autodaily()\n",
    "    response = requests.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1').text.strip()\n",
    "    date = soup.find('time').text.strip()\n",
    "    published_date = convert_string(date)\n",
    "    article = soup.find('div', class_ = 'article-detail')\n",
    "    for script_or_style in article(['script', 'style','iframe']):\n",
    "                script_or_style.decompose()\n",
    "    related_item = article.find('div', class_ = 'item-relate')\n",
    "    related_item.decompose()\n",
    "    author_item = article.find_all(recursive = True)[-2:]\n",
    "    for i in author_item:\n",
    "        i.decompose()\n",
    "    caption_text_list = article.find_all('em')\n",
    "    tags_to_remove = article.find_all(['a', 'span'])\n",
    "    for tag in tags_to_remove:\n",
    "        # Extract the text from the tag\n",
    "        tag_text = tag.get_text()\n",
    "        # Replace the tag with its text content\n",
    "        tag.replace_with(tag_text)\n",
    "        tag.text.strip()\n",
    "    for i in article.find_all('img'):\n",
    "        #i.attrs = ['class', 'alt', 'src', 'data-original']\n",
    "        del i['onclick']\n",
    "        del i['style']\n",
    "        del i['class']\n",
    "    img_list = article.find_all('img')\n",
    "    n_img = len(img_list)\n",
    "    #print(len(caption_text_list))\n",
    "    for i in range(0,n_img):\n",
    "        caption_start = NavigableString(\"[caption id=\\\"\\\" align=\\\"aligncenter\\\" width=\\\"800\\\"]\")\n",
    "        try:\n",
    "            caption_text = NavigableString(caption_text_list[i].get_text())\n",
    "        except IndexError:\n",
    "            caption_text = ''\n",
    "        caption_end = NavigableString(\"[/caption]\")\n",
    "        # Insert the custom tags and caption text around the <img> tag\n",
    "        img_list[i].insert_before(caption_start)\n",
    "        img_list[i].insert_after(caption_end)\n",
    "        img_list[i].insert_after(caption_text) \n",
    "        img_list[i]['src'] = img_list[i]['data-src']\n",
    "    for i in caption_text_list:\n",
    "        i.decompose()\n",
    "        #print(img_list[i]['src'])\n",
    "        #caption_text_list[i].decompose()\n",
    "    for i in article.find_all(recursive = True):\n",
    "        try:\n",
    "            del i['onclick']\n",
    "            del i['id']\n",
    "            del i['class']\n",
    "            del i['style']\n",
    "            del i['href']\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        #try:\n",
    "            #print(i)\n",
    "            \n",
    "        except TypeError:\n",
    "            continue\n",
    "    for i in article.find_all('img'):\n",
    "        i['class'] = \"aligncenter\"\n",
    "        i['width'] = 800\n",
    "        i['height'] = 400\n",
    "    for item in article.find_all('div'):\n",
    "        if item.string ==\"\":\n",
    "            item.decompose()\n",
    "    source_tag = soup.new_tag('i') \n",
    "    source_tag.string = \"Nguồn: autodaily.vn\"  # Set the content of <i> tag\n",
    "    \n",
    "    # Append the <i> tag as the last child of the <article> tag\n",
    "    article.append(source_tag)\n",
    "\n",
    "    return article, title, published_date\n",
    "def get_post(url):\n",
    "    try:\n",
    "        content,title,published_date = get_content_autodaily(url)\n",
    "        return content,title,published_date\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "def get_list_url(cate_url):\n",
    "    response = requests.get(cate_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    featured_posts = soup.find('div', class_= 'news-list')\n",
    "    list = featured_posts.find_all('div', class_ = 'news-item')\n",
    "    urls = []\n",
    "    for i in list:\n",
    "        path = i.find('picture').find('a')['href']\n",
    "        url = 'https://xedoisong.vn' + path\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "def filter_list(urls):\n",
    "    filtered_urls = []\n",
    "    crawl_time = datetime.fromtimestamp(time.time()-7*24*3600)\n",
    "    for i in urls:\n",
    "        response = requests.get(i)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        try:\n",
    "            #wrapper = soup.find('div', class_ = \"col780 left clearafter\")\n",
    "            #date_posted = wrapper.find('p',class_ ='news-time left').text.strip()\n",
    "            #date_posted_norm = convert_time_string(date_posted)\n",
    "            date_str = soup.find('time').text.strip()\n",
    "            #print(date_str)\n",
    "            date_posted_norm = get_time_string(date_str)[0]\n",
    "            if ( (date_posted_norm.day == crawl_time.day) and (date_posted_norm.month == crawl_time.month) and (date_posted_norm.year == crawl_time.year) ):\n",
    "                filtered_urls.append(i)\n",
    "                #print(i)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return filtered_urls\n",
    "#add list url to json\n",
    "#add list url to json\n",
    "def add_list(web_json_obj):\n",
    "    for i in list(web_json_obj['urls'].keys()):\n",
    "        for j in list(web_json_obj['urls'][i]['sub-category'].keys()):  \n",
    "            urls = get_list_url(web_json_obj['urls'][i]['sub-category'][j]['url'])\n",
    "            print(i,j,web_json_obj['urls'][i]['sub-category'][j]['url'])\n",
    "            web_json_obj['urls'][i]['sub-category'][j]['url_list'] = filter_list(urls)\n",
    "# add post content from get content function to json object\n",
    "def add_post(web_json_obj):\n",
    "    for i in list(web_json_obj['urls'].keys()):\n",
    "        for j in list(web_json_obj['urls'][i]['sub-category'].keys()):\n",
    "            web_json_obj['urls'][i]['sub-category'][j]['content'] = {}\n",
    "            list_key = [v for v in range(0,len(web_json_obj['urls'][i]['sub-category'][j]['url_list']))]\n",
    "            for u in list_key:\n",
    "                web_json_obj['urls'][i]['sub-category'][j]['content'][u] = {}\n",
    "                if u != \"\":\n",
    "                    web_json_obj['urls'][i]['sub-category'][j]['content'][u]['text'] ,web_json_obj['urls'][i]['sub-category'][j]['content'][u]['title'],web_json_obj['urls'][i]['sub-category'][j]['content'][u]['published_date'] = get_post(web_json_obj['urls'][i]['sub-category'][j]['url_list'][u])\n",
    "                    print(i,j,web_json_obj['urls'][i]['sub-category'][j]['cate_id'],web_json_obj['urls'][i]['sub-category'][j]['name'],web_json_obj['urls'][i]['sub-category'][j]['name'],web_json_obj['urls'][i]['sub-category'][j]['content'][u]['title'],web_json_obj['urls'][i]['sub-category'][j]['url_list'][u])\n",
    "#add all necessary information to json object\n",
    "def get_news_autodaily():\n",
    "    _autodaily= {\n",
    "            \"home_page\":\"https://autodaily.vn/\",\n",
    "            \"urls\":{\n",
    "                \"Ô tô\":\n",
    "                {\n",
    "                 \"url\":\"https://autodaily.vn/#\",\n",
    "                 \"sub-category\":{  \n",
    "                    0:{\"name\":\"Ô tô\",\n",
    "                     \"url\":\"https://autodaily.vn/\",\n",
    "                     \"cate_id\":58,\n",
    "                      \"url_list\" : []},\n",
    "                 }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "#\n",
    "    add_list(_autodaily)\n",
    "    add_post(_autodaily)\n",
    "    return _autodaily\n",
    "#send post content to wordpress via endpoint\n",
    "def send_post_to_5goals(title,content,category_id,published_date):\n",
    "    # URL of the API endpoint (this is a placeholder and needs to be replaced with the actual URL)\n",
    "    url = \"https://api2023.5goal.com/wp-json/custom/createPost\"\n",
    "    \n",
    "    # Data to be sent in the POST request\n",
    "    data = {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"category_id\": category_id,\n",
    "        \"token\": '5goalvodichcmnl',  # Replace with your actual access token\n",
    "        \"published_date\": published_date,\n",
    "        \"domain\":\"bongda24h\"\n",
    "          # Replace with the actual category ID as required\n",
    "    }\n",
    "    \n",
    "    # Sending the POST request\n",
    "    response = requests.post(url, data=data)\n",
    "    \n",
    "    # Checking the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"The post was successfully created.\")\n",
    "        print(\"Response:\", response.text)  # Prints the response text from the server\n",
    "    else:\n",
    "        print(f\"Failed to create the post. Status code: {response.status_code}\")\n",
    "def main():\n",
    "    _autodaily = get_news_autodaily()\n",
    "    for i in list(_autodaily['urls'].keys()):\n",
    "    #web_24h_com_vn2['url'][i]['cate_id']\n",
    "        for j in list(_autodaily['urls'][i]['sub-category']):\n",
    "            url_list =  _autodaily['urls'][i]['sub-category'][j]['url_list']\n",
    "            print(url_list)\n",
    "            for t in range(0,len(url_list)):\n",
    "                content = _autodaily['urls'][i]['sub-category'][j]['content'][t]['text']\n",
    "                title = _autodaily['urls'][i]['sub-category'][j]['content'][t]['title']\n",
    "                published_date = _autodaily['urls'][i]['sub-category'][j]['content'][t]['published_date']\n",
    "                cate_id = _autodaily['urls'][i]['sub-category'][j]['cate_id']\n",
    "                print(title, url_list[t])\n",
    "                send_post_to_5goals(title,str(content), cate_id, published_date)\n",
    "                time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ccbbc0-7791-4550-a926-c59b1643ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_url(cate_url):\n",
    "    response = requests.get(cate_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    featured_posts = soup.find('div', class_= 'news-list')\n",
    "    list = featured_posts.find_all('div', class_ = 'news-item')\n",
    "    urls = []\n",
    "    for i in list:\n",
    "        path = i.find('picture').find('a')['href']\n",
    "        url = 'https://xedoisong.vn' + path\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad4bbc3-683d-463b-9e4d-4e4475741263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://xedoisong.vn/gsm-va-tuyet-chieu-marketing-mo-hinh-5a-loi-hai-cua-ty-phu-pham-nhat-vuong-cho-vinfast',\n",
       " 'https://xedoisong.vn/porsche-copy-y-tuong-cua-con-gai-nha-sang-lap-hang-tao-ra-911-turbo-hang-thua-the-he-moi-nhat',\n",
       " 'https://xedoisong.vn/xe-dien-trung-quoc-byd-vao-chung-ket-giai-thuong-car-of-the-year-2024-tai-troi-au-doi-dau-voi-hang-loat-ten-tuoi-lon',\n",
       " 'https://xedoisong.vn/khach-viet-yen-tam-mua-hyundai-palisade-hien-gio-se-khong-so-lac-hau-it-nhat-la-vai-nam-nua-nhung',\n",
       " 'https://xedoisong.vn/5-mau-xe-hoi-vua-ra-mat-viet-nam-thang-qua-toan-xe-da-dung-cao-cap-tien-ty-tru-vinfast-vf-7',\n",
       " 'https://xedoisong.vn/gia-ban-du-kien-volkswagen-viloran-tu-1968-ty-dong-cao-hon-100-trieu-kia-carnival-ban-cao-nhat',\n",
       " 'https://xedoisong.vn/mo-to-xe-may-moi-ve-viet-nam-thang-11-nhieu-piaggio-va-vespa-phan-khoi-lon-toan-tien-ty',\n",
       " 'https://xedoisong.vn/bo-doi-mpv-cua-hyundai-nhan-uu-dai-stargazer-giam-hon-120-trieu-va-custin-giam-gan-30-trieu',\n",
       " 'https://xedoisong.vn/kho-tin-mo-to-phan-khoi-lon-trung-quoc-lai-co-the-dep-the-nay-boc-lop-vo-ra-moi-biet-ly-do',\n",
       " 'https://xedoisong.vn/chi-tiet-mau-cao-cao-tf-250-x-dau-tien-cua-triumph-khong-thua-kem-bat-ky-ong-lon-nao-trong-phan-khuc',\n",
       " 'https://xedoisong.vn/toan-ban-xe-300cc-tai-viet-nam-hang-trung-quoc-zontes-tu-nghien-cuu-dong-co-3-xi-lanh-cho-2-mau-phan-khoi-lon-dau-yamaha-va-triumph',\n",
       " 'https://xedoisong.vn/quai-vat-ktm-1390-super-duke-r-hoan-toan-moi-trinh-lang',\n",
       " 'https://xedoisong.vn/xe-dien-luxeed-s7-cua-huawei-chinh-thuc-ra-mat-bat-ngo-kha-nang-sac-15-phut-di-duoc-430km',\n",
       " 'https://xedoisong.vn/hoc-tap-doi-tac-chau-au-ktm-hang-mo-to-trung-quoc-cfmoto-cung-tung-ban-hieu-nang-cao-gp-cho-naked-bike-800nk']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_list_url('https://xedoisong.vn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c323bd-39b9-4002-a4c9-be240b0de1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://xedoisong.vn/5-mau-xe-hoi-vua-ra-mat-viet-nam-thang-qua-toan-xe-da-dung-cao-cap-tien-ty-tru-vinfast-vf-7')\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "title = soup.find('h1').text.strip()\n",
    "date = soup.find('time').text.strip()\n",
    "published_date = convert_string(date)\n",
    "article = soup.find('div', class_ = 'news-content')\n",
    "bottom_div = article.find_all(recursive = True)[-3:]\n",
    "for i in bottom_div:\n",
    "    i.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bb31a-a0d4-4b16-8a51-6720e145162c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
